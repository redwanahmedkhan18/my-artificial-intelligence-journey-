{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b956b970",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ Vectors â†’ Python â†’ Machine Learning (From Scratch)\n",
    "\n",
    "This notebook connects **vector mathematics** directly to **Machine Learning concepts**  \n",
    "using **clear theory + Python implementations**.\n",
    "\n",
    "You will learn:\n",
    "- What vectors are in ML\n",
    "- How scalars, vectors, dot products, norms work\n",
    "- How ML predictions, loss, gradients, and updates are built from vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba1934",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Scalars vs Vectors in ML\n",
    "\n",
    "- **Scalar**: single number (learning rate, bias, loss)\n",
    "- **Vector**: ordered list of numbers (features, weights, gradients)\n",
    "\n",
    "In ML:\n",
    "- Learning rate â†’ scalar  \n",
    "- Bias â†’ scalar  \n",
    "- Features â†’ vector  \n",
    "- Weights â†’ vector  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4396d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.01     # scalar\n",
    "bias = 2.0               # scalar\n",
    "\n",
    "x = np.array([1200, 3, 10])   # feature vector (house)\n",
    "w = np.array([0.5, 1.2, -0.3]) # weight vector\n",
    "\n",
    "x, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a339f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Vector Dimensions\n",
    "\n",
    "A vector with *n* values is an **n-dimensional vector**.\n",
    "\n",
    "Examples:\n",
    "- 3 features â†’ 3D vector\n",
    "- 784 pixels â†’ 784D vector (MNIST)\n",
    "\n",
    "Dimension = number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a21e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Vector dimension:\", x.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bb830",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dot Product â†’ Prediction (Core ML Operation)\n",
    "\n",
    "Dot product computes a **weighted sum**.\n",
    "\n",
    "\\[\n",
    "y = w Â· x\n",
    "\\]\n",
    "\n",
    "This is the heart of:\n",
    "- Linear Regression\n",
    "- Single neuron in Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d999e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = np.dot(w, x)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5fba5",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Bias Term\n",
    "\n",
    "Final prediction:\n",
    "\\[\n",
    "y = w Â· x + b\n",
    "\\]\n",
    "\n",
    "Bias allows the model to shift predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = np.dot(w, x) + bias\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee49d05",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Vector Norms\n",
    "\n",
    "### L2 Norm (Euclidean Length)\n",
    "\\[\n",
    "||x||_2 = \\sqrt{\\sum x_i^2}\n",
    "\\]\n",
    "\n",
    "Used in:\n",
    "- Regularization (Ridge)\n",
    "- Distance measurement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l2_norm = np.linalg.norm(x)\n",
    "l2_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44764e7",
   "metadata": {},
   "source": [
    "\n",
    "### L1 Norm\n",
    "\\[\n",
    "||x||_1 = \\sum |x_i|\n",
    "\\]\n",
    "\n",
    "Used in:\n",
    "- LASSO (feature selection)\n",
    "- Sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5143acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l1_norm = np.sum(np.abs(x))\n",
    "l1_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b9005",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Feature Scaling\n",
    "\n",
    "Scaling vectors improves training stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_scaled = x / np.linalg.norm(x)\n",
    "x_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e986c55",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Cosine Similarity\n",
    "\n",
    "Measures **direction similarity**, not magnitude.\n",
    "\n",
    "\\[\n",
    "cos(Î¸) = \\frac{xÂ·y}{||x|| ||y||}\n",
    "\\]\n",
    "\n",
    "Used in:\n",
    "- NLP embeddings\n",
    "- Semantic search\n",
    "- Recommendation systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a726bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "x2 = np.array([1300, 3, 12])\n",
    "cosine_similarity(x, x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78df0ba",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Loss Function (Scalar Output)\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\\[\n",
    "L = (y_{true} - y_{pred})^2\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = 250\n",
    "loss = (y_true - y_pred) ** 2\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33c206",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Gradient (Vector)\n",
    "\n",
    "Gradient shows **how to change weights** to reduce loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f68b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad_w = -2 * (y_true - y_pred) * x\n",
    "grad_b = -2 * (y_true - y_pred)\n",
    "\n",
    "grad_w, grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ff16e",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Gradient Descent Update\n",
    "\n",
    "\\[\n",
    "w_{new} = w - Î· âˆ‡L\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_new = w - learning_rate * grad_w\n",
    "b_new = bias - learning_rate * grad_b\n",
    "\n",
    "w_new, b_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1c984",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Mini Training Loop (Vector-Based ML)\n",
    "\n",
    "This is **machine learning using only vectors and scalars**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = np.array([0.1, 0.1, 0.1])\n",
    "b = 0.0\n",
    "lr = 0.001\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = np.dot(w, x) + b\n",
    "    loss = (y_true - y_pred) ** 2\n",
    "    \n",
    "    grad_w = -2 * (y_true - y_pred) * x\n",
    "    grad_b = -2 * (y_true - y_pred)\n",
    "    \n",
    "    w -= lr * grad_w\n",
    "    b -= lr * grad_b\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss={loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c5247",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸŽ¯ Key Takeaway\n",
    "\n",
    "> **Machine Learning starts with vectors + scalars.  \n",
    "Matrices simply scale this to many samples.**\n",
    "\n",
    "You now understand:\n",
    "- How ML predictions work\n",
    "- Why gradients are vectors\n",
    "- Why learning rate is scalar\n",
    "- How theory maps directly to code\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
